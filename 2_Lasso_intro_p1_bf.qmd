---
title: "Introduction of Lasso (Part I)"
author: "Luhan Tang"
format: html
editor: visual
---

## 

### Final Project

**Author**: Luhan Tang **Course**: STAT206

# Background

The **Least Absolute Shrinkage and Selection Operator (LASSO)**, proposed by Tibshirani (1996), is a regression method designed to improve prediction accuracy and interpretability of statistical models.

In traditional Ordinary Least Squares (OLS) regression, the model often has low bias but suffers from high variance, especially with a large number of predictors. This can result in overfitting and decreased generalization performance. LASSO addresses these issues by applying a constraint on the sum of the absolute values of the regression coefficients, encouraging sparsity in the model.

# Recall: Least Squares Regression

First and foremost, I want to bring you back to the Least Squares Regression. It is the basis of LASSO Regression.

Suppose we are given $n$ observations of the form $(x_i, y_i)$, $i = 1, \dots, n$, where each $x_i \in \mathbb{R}^d$ denotes a feature vector and $y_i \in \mathbb{R}$ an associated response value. Let $X \in \mathbb{R}^{n \times d}$ denote the predictor matrix (whose $i^{th}$ row is $x_i$) and $Y \in \mathbb{R}^n$ denote the response vector. Recall that the least squares regression coefficients of $Y$ on $X$ are given by solving $\min_{\beta} \|Y - X\beta\|_2^2$. When $d \leq n$ and $\text{rank}(X) = d$, this produces the unique solution $\hat{\beta} = (X^\top X)^{-1} X^\top Y$. The fitted values (i.e., in-sample predictions) are $X\hat{\beta} = X(X^\top X)^{-1} X^\top Y = P_X Y$, where $P_X = X(X^\top X)^{-1} X^\top$ denotes the projection onto the column space of $X$.

$$
\min_{\beta} \|Y - X\beta\|_2^2.
$$

## Principle

When $d \leq n$ and $\text{rank}(X) = d$, this produces the unique solution:

$$
\hat{\beta} = (X^\top X)^{-1} X^\top Y.
$$

The fitted values (i.e., in-sample predictions) are:

$$
X\hat{\beta} = X(X^\top X)^{-1} X^\top Y = P_X Y,
$$ where $P_X = X(X^\top X)^{-1} X^\top$ denotes the projection onto the column space of $X$.

## Trouble in OLS when in High Dimensions

As we just saw, the risk of least squares regression degrades as $d$ grows close to $n$, the out-of-sample risk actually diverges at $d = n$.

Meanwhile, the least squares estimator itself is not even well-defined when $d > n$, in that the optimization problem $\min_{\beta} \|Y - X\beta\|_2^2.$ does not have a unique solution. In this case, any vector of the form $$
\hat{\beta} = (X^\top X)^+ X^\top Y + \eta, \quad \text{where } \eta \in \text{null}(X),
$$

solves it, where we write $A^+$ to denote the generalized inverse of a matrix $A$, and $\text{null}(A)$ to denote its null space.

If all we care about is out-of-sample prediction, then this is not the end of the story for least squares—it turns out that taking $\eta = 0$ in (5), which yields the minimum $\ell_2$ norm least squares solution, can still have interesting predictive properties when $d > n$.

But if we additionally care about the estimated coefficients themselves, then it really is the end of the road for least squares. This is because, for any $\hat{\beta}$ of the form (5) with $\hat{\beta}_j > 0$ for some component $j$, we can always find another $\hat{\beta}$ of the form (5) with $\hat{\beta}_j < 0$. So we cannot even consistently interpret the sign of any estimated coefficient (let alone its magnitude).

## LASSO

Lasso regression, or Least Absolute Shrinkage and Selection Operator regression, is a regularization technique that improves predictive accuracy by shrinking coefficient estimates toward a central point. By applying an L1 regularization penalty, Lasso simultaneously performs variable selection and coefficient estimation, setting some coefficients to exactly zero, which simplifies models by eliminating irrelevant predictors. This makes it particularly effective for high-dimensional data, addressing multicollinearity and automating feature selection, especially when working with a large number of predictors.

### Regularization

#### Regularization in Least Squares Setting

At a high level, it allows for the production of nontrivial coefficient estimates and can often yield better predictions. While traditional views hold that regularization almost always improves predictions, recent research in overparameterization reveals nuances in its effects. The effectiveness of regularization depends strongly on the specific characteristics of the prediction problem.

In the context of least squares regression, traditional approaches to regularization are expressed in two forms:

-   **Constrained form**:\
    Minimize $\| Y - X \beta \|_2^2$ subject to $\beta \in C$.

-   **Penalized form**:\
    Minimize $\| Y - X \beta \|_2^2 + h(\beta)$.

Here: 
- $C$ represents a (typically convex) set. 
- $h$ is a (typically convex) penalty function.\
For instance, $C = \{ \beta : \| \beta \| \leq t \}$ is the sublevel set of a norm $\|\cdot\|$, and $h(\beta) = \lambda \|\beta\|$ is a nonnegative multiple of the norm.

The constrained and penalized forms are equivalent under convex duality. Specifically, for any $t \geq 0$, there exists a value $\lambda \geq 0$ such that the solution $\hat{\beta}$ to the constrained problem also solves the penalized problem, and vice versa.

### Canonical Regularizers:  $\ell_1$

-   $\ell_1$ norm: $\|\beta\|_1 = \sum_{j=1}^d |\beta_j|$

- $\ell_1$ norm corresponds to **lasso regression**. 

### 1. Formula

LASSO minimizes the residual sum of squares subject to a penalty on the absolute values of the coefficients:

$$
\min_{\beta} \left\{ \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2 \right\}, \quad \text{subject to} \quad \sum_{j=1}^p |\beta_j| \leq t
$$

Where:

-   $y_i:$ Response variable.

-   $x_{ij}:$ Predictor variables.

-   $\beta_j:$ Coefficients to be estimated.

-   $t:$ Tuning parameter that controls the amount of shrinkage applied.

By penalizing the absolute size of the coefficients, LASSO tends to set some coefficients to exactly zero, effectively selecting a simpler subset of predictors.

### LASSO Regression Objective Function

The LASSO (Least Absolute Shrinkage and Selection Operator) regression minimizes the following objective function: $$
\min_{\beta} \left\{ \frac{1}{2n} \sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j \right)^2 + \lambda \sum_{j=1}^p |\beta_j| \right\}
$$ Where: 

- $y_i$: Response variable for the $i$-th observation. 
- $x_{ij}$: Predictor variable for the $i$-th observation and $j$-th feature. 
- $\beta_j$: Coefficients to be estimated for the $j$-th predictor. 
- $\lambda$: Regularization parameter controlling the amount of shrinkage. 
- $n$: Number of observations. 
- $p$: Number of predictors.

-   **Residual Sum of Squares (RSS)**: The first term measures the goodness of fit of the model to the data.
-   **L1 Regularization Term**: The second term penalizes the absolute values of the coefficients, encouraging sparsity.
-   **Tuning Parameter (**$\lambda$): Controls the trade-off between model complexity and goodness of fit.

As $\lambda$ increases, more coefficients are shrunk to zero, effectively performing feature selection.

### 2. Objective of LASSO Regression

The objective of LASSO regression is to determine the coefficients that minimize the following objective function:

Objective Function = $RSS + L_1$

### 3. Shrinking Coefficients in LASSO Regression

LASSO regression incorporates the L1 regularization term, which shrinks the coefficients towards zero. When the tuning parameter $\lambda$ is sufficiently large, some coefficients are driven exactly to zero.

This unique property of LASSO regression makes it particularly useful for feature selection, as variables with zero coefficients are effectively excluded from the model.

### 4. Tuning Parameter

The regularization parameter $\lambda$ plays a crucial role in LASSO regression:

-   A larger $\lambda$ increases the amount of regularization, pushing more coefficients towards zero.
-   A small $\lambda$ reduces the regularization effect, allowing more variables to retain non-zero coefficients.

### 5. Model Fitting

To estimate the coefficients in LASSO regression, optimization algorithms are employed to minimize the objective function. One commonly used algorithm is **Coordinate Descent**, which iteratively updates each coefficient while keeping the others fixed.

### Example in R

```{r}
# Load the necessary libraries
#install.packages("rms")
#install.packages("glmnet")
#install.packages("caret")
library(rms)
library(glmnet) # For lasso regression
library(caret)  # For additional model utilities

# Load the dataset and handle missing values
data <- survival::lung  # Load the dataset directly from the survival package
data <- na.omit(data)  # Remove rows with missing values

# If a variable has 3 categories, create dummy variables
# Assuming a categorical variable `X` has levels A, B, C
# Convert it to two dummy variables
# data$X_B <- ifelse(data$X == "B", 1, 0)
# data$X_C <- ifelse(data$X == "C", 1, 0)

# Convert data to a matrix format for glmnet
# Predictors are columns 4 to 10
X <- as.matrix(data[, 4:10])

# Response variable is column 3
Y <- as.matrix(data[, 3])

# Fit a lasso regression model
# alpha = 1 indicates lasso regression; alpha = 0 would indicate ridge regression
lasso <- glmnet(X, Y, alpha = 1, family = "binomial", nlambda = 100)

# If alpha = 0, glmnet performs ridge regression.
# `nlambda = 100` ensures 100 iterations to find the optimal lambda.
print(lasso)

# Plot the coefficients against the log(lambda)
plot(lasso, xvar = "lambda", label = TRUE)
title(main = "LASSO Path: Coefficients vs Log(Lambda)", xlab = "Log(Lambda)", ylab = "Coefficients")

# Cross-validation for optimal lambda
set.seed(42)  # For reproducibility
cv_lasso <- cv.glmnet(X, Y, alpha = 1, family = "binomial", nfolds = 10)

# Plot cross-validation results
plot(cv_lasso)
title(main = "Cross-Validation for LASSO", xlab = "Log(Lambda)", ylab = "Cross-Validation Error")

# Optimal lambda
optimal_lambda <- cv_lasso$lambda.min
cat("Optimal Lambda: ", optimal_lambda, "\n")

# Coefficients at optimal lambda
lasso_coef <- coef(cv_lasso, s = "lambda.min")
print(lasso_coef)
```

# Reference Material

> Tibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. *Journal of the Royal Statistical Society: Series B (Methodological)*, 58(1), 267–288.

For further exploration, the following references provide comprehensive insights into the theory and applications of LASSO regression:

1.  **Lecture Notes by Tibshirani**: Available at [LASSO Regression Notes](https://www.stat.berkeley.edu/~ryantibs/statlearn-s23/lectures/lasso.pdf).
2.  **University of Chicago Materials**: Accessible at [Stat 224 Lecture 18](https://www.stat.uchicago.edu/~yibi/teaching/stat224/L18.pdf).
